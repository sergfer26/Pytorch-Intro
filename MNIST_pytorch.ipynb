{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Digit Recognition Using Pytorch \n",
    "\n",
    "En este notebook discutiremos el uso de redes neuronales para desarrollar un clasificador que reconozca digitos hechos a mano. Desarrollaremos de cero un clasificador usando pytorch y el conjuto de datos de MNIST. \n",
    "\n",
    "# Dataset\n",
    "\n",
    "Usaremos la popular base de datos de MNIST. Este conjunto de datos consiste de una colleción de $70,000$ digitos escritos a mano. Pytorch nos ofrece los datos ya preparados y divididos, un $90\\%$ para entrenamiento y el resto para pruba. Esto representa una ventaja pues sólo es necesario escribir unas cuantas lineas de código como se verá después.\n",
    "\n",
    "Usaremos las siguientes bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch # biblioteca similar a python para manipular tensores\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms # módulo con transformaciones de tensores\n",
    "from torch import nn, optim # son los modulos de las arquitecturas de redes y los optimizadores, respectivamente\n",
    "import torch.nn.functional as F # módulo para funciones usadas en las redes\n",
    "from torchsummary import summary # muestra la información de un modelo\n",
    "from tqdm import tqdm # nos permite generar barras de progeso para el entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos la siguiente pipeline de transformaciones para poder alimentar al modelo con los datos en la forma que requiere. Usareamos el módulo ``transforms``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.5,),(0.5,)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ``ToTensor()``: Convierte la imagen en arreglos numéricos que entiende la computadora. Separa a cada pixel de la imagen en los canales rojo, verde y azul y le da un valor numérico de acuerdo al brillo que va de 0 a 255. El producto es un tensor que representa a la imagen en ``torch``.\n",
    "\n",
    "* ``Normalize()``: normaliza a las entradas de un tensor con los parametros de media y desviación estandar. \n",
    "\n",
    "Ahora descargaremos la base de datos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = MNIST('PATH_TO_STORE_TRAINSET', \n",
    "                         download=True, train=True, transform=transform)\n",
    "\n",
    "testset = MNIST('PATH_TO_STORE_TRAINSET', \n",
    "                         download=True, train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora será ncesario separar una parte del conjunto de entrenamiento para usarlo en la validación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 0.2 # parte para validar\n",
    "n = len(trainset)\n",
    "indices = list(range(n))\n",
    "np.random.shuffle(indices) # revolvemos los indices\n",
    "split = int(np.floor(valid_size * n)) \n",
    "train_idx, valid_idx = indices[split:], indices[:split] # seprarmos los indices\n",
    "\n",
    "train_set = torch.utils.data.Subset(trainset, train_idx) # tomamos un subconjunto de acuerdo a los indices\n",
    "val_set = torch.utils.data.Subset(trainset, valid_idx)\n",
    "n_train = len(train_set)\n",
    "n_val = len(val_set) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya con los conjuntos de datos almacenados y separados para el entrenamiento, validación y prueba, será necesario darles la estructura de un objeto de la clase ``DataLoader``, que permite separar la conjuto de datos por batches y permite crear iteradores simples o multi-proceso sobre el conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set,\n",
    "                                          batch_size=32, shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_set,\n",
    "                                          batch_size=32, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(testset,\n",
    "                                          batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploración y análisis del Dataset\n",
    "\n",
    "En esta sección haremos una exploración y análisis de las imagenes de nuestro conjunto de datos. En la siguiente instrucción desempaquetamos las etiquetas y las imagenes del conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora veamos las dimensiones de las imagenes y las etiquetas. La primera dimensión indica el batch size, cada elemento del bach es una imagen de $28 \\times 28$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente instrucción solo es ilustrativa y muestra como es una imagen del conjunto con su etiqueta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dígito:  7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 10\n",
    "plt.imshow(images[index].numpy().squeeze(), cmap='gray_r'); \n",
    "print('dígito: ',int(labels[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura de la red\n",
    "\n",
    "Ahora procedemos a la definición del modelo. Proponemos una red feedforward con 2 capas intermedias, con función de activación relu. Como la tarea es de clasificar la capa de salida consitira de 10 nodos, una por cada clase. \n",
    "\n",
    "Definiremos una clase llamada ``Classifier``, esta clase hereda de ``nn.Module`` que es una clase abstracta que modela a las redes neuronales, usaremos esta estructura para definir una red neuronal con dos capas intermedias de tamaño $128$ y $64$ respectivamente. Después de definir la estructura de las capas de salida e intermedias como atributos, les damos un orden en el método ``forward`` donde las conectamos y les aplicamos la función de activación, finalmente agragamos la capa de salida. Note que la capa de salida no tiene la función de activación usual, más adelante veremos porque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28 * 28\n",
    "hidden_sizes = [128, 64]\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size=input_size, hidden_sizes=hidden_sizes , classes=10):\n",
    "        super(Classifier, self).__init__()\n",
    "        s1, s2 = hidden_sizes\n",
    "        self.linear1 = nn.Linear(input_size, s1) # capa intermedia 1\n",
    "        self.linear2 = nn.Linear(s1, s2) # capa intermedia 2\n",
    "        self.linear3 = nn.Linear(s2, classes)  # capa de salida\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaramos al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llamamos a la función ``summary`` para ver información acerca del modelo, en la seunda entrada deben de ponerse la dimensión de los atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1               [-1, 1, 128]         100,480\n",
      "            Linear-2                [-1, 1, 64]           8,256\n",
      "            Linear-3                [-1, 1, 10]             650\n",
      "================================================================\n",
      "Total params: 109,386\n",
      "Trainable params: 109,386\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.42\n",
      "Estimated Total Size (MB): 0.42\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (1, 28 * 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier(\n",
      "  (linear1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (linear3): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizador y función de costos\n",
    "\n",
    "Discutiremos más adelante la capa de salida de la arquitectura y la función costos (*CIFAR10_pytorch.ipynb*). Sólo es necesario notar que no es necesario usar la función softmax pues ya viene integrada en la función de costos. Tampoco es necesario que las etiquetas tengan la estructura *one hot*, pues la función *CrossEntropyLoss* nos evita esa tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El optimizador es proporcionado por ``torch.optim``, es el que realizará la tarea de descenso de gradiente y actualizará los pesos por medio del métdodo de *backpropagation*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento\n",
    "\n",
    "Ya estamos preparados para continuar con el entrenamiento para ello definiremos la función ``training_loop``. En la siguiente función la red neuronal itererá sobre conjunto de entrenamiento y lo integrará con el optimizador que hará su tarea. Note que a diferencia de Keras es necesario decirle a la red neuronal cuando va a tener que darle seguiemiento al gradiente en cada paso para después hacer backpropagation.\n",
    "\n",
    "No sólo usaremos esta función para el entrenamiento si no que también para la validación. Es necesario decirle a la red cuando vamos a hacer el proceso de validación y no darle seguimiento del grandiente para eso sirve la bandera ``valid``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(train_loader, model, optimizer, loss_function, pbar, valid=False):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    if valid: \n",
    "        model.eval() # modo de validación del modelo \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        X, Y = data\n",
    "        X = X.view(images.shape[0], -1) # transformamos los datos para la red\n",
    "        # print(X.shape)\n",
    "        if not valid:\n",
    "            optimizer.zero_grad() # reinicia el gradiente\n",
    "            \n",
    "        pred = model(X)\n",
    "        #import pdb; pdb.set_trace()\n",
    "        loss = loss_function(pred, Y)\n",
    "        if not valid:\n",
    "            loss.backward() # cálcula las derivadas \n",
    "            optimizer.step() # paso de optimización \n",
    "            \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        avg_loss = running_loss/(i + 1)\n",
    "        \n",
    "        probs = F.softmax(pred, 1)\n",
    "        label = torch.argmax(probs, dim=1)\n",
    "        correct += torch.sum(label == Y).item()\n",
    "        total += Y.shape[0]\n",
    "        acc = correct/total\n",
    "        \n",
    "        pbar.set_postfix(avg_loss='{:.4f}'.format(avg_loss), acc='{:.4f}'.format(acc))\n",
    "        pbar.update(Y.shape[0])\n",
    "        \n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora establecemos la cantidad de épocas y generamos las listas donde guardaremos el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "EPOCHS = 15\n",
    "\n",
    "train_time = 0\n",
    "epoch_loss = []\n",
    "epoch_acc = []\n",
    "val_loss = []\n",
    "val_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "len(train_loader) * 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Juntamos todo y empezamos el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 - training: 100%|██████████| 48000/48000 [00:12<00:00, 3695.36it/s, acc=0.8813, avg_loss=0.3914]\n",
      "Epoch 1/15 - validation: 100%|██████████| 12000/12000 [00:02<00:00, 4383.01it/s, acc=0.9274, avg_loss=0.2411]\n",
      "Epoch 2/15 - training: 100%|██████████| 48000/48000 [00:14<00:00, 3341.34it/s, acc=0.9386, avg_loss=0.1995]\n",
      "Epoch 2/15 - validation: 100%|██████████| 12000/12000 [00:02<00:00, 4402.97it/s, acc=0.9376, avg_loss=0.2003]\n",
      "Epoch 3/15 - training:  52%|█████▏    | 24992/48000 [00:07<00:07, 3166.55it/s, acc=0.9526, avg_loss=0.1532]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-dffef30a905e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mpbar_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch + 1}/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' - training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mpbar_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'0.0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'0.0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mloss_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpbar_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mtrain_time\u001b[0m \u001b[0;34m+=\u001b[0m  \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-286cf552077a>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(train_loader, model, optimizer, loss_function, pbar, valid)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# reinicia el gradiente\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;31m#import pdb; pdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-bd0f69599b8a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1608\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1609\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1611\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "    with tqdm(total = n_train, position=0) as pbar_train:\n",
    "        pbar_train.set_description(f'Epoch {epoch + 1}/'+str(EPOCHS)+' - training')\n",
    "        pbar_train.set_postfix(avg_loss='0.0', acc='0.0')\n",
    "        loss_train, acc_train = training_loop(train_loader, model, optimizer, criterion, pbar_train, valid=False)\n",
    "        train_time +=  time.time() - start_time\n",
    "    with tqdm(total = n_val, position=0) as pbar_val:\n",
    "        pbar_val.set_description(f'Epoch {epoch +1}/'+str(EPOCHS)+' - validation')\n",
    "        pbar_val.set_postfix(avg_loss='0.0', acc='0.0')\n",
    "        loss_val, acc_val = training_loop(val_loader, model, None, criterion, pbar_val, valid=True)\n",
    "    \n",
    "    epoch_loss.append(loss_train)\n",
    "    epoch_acc.append(acc_train)\n",
    "    val_loss.append(loss_val)\n",
    "    val_acc.append(acc_val)\n",
    "\n",
    "print(\"--- %s minutes ---\", train_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendimiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_acc)\n",
    "plt.plot(val_acc)\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_loss)\n",
    "plt.plot(val_loss)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "for i, data in enumerate(test_loader, 0):\n",
    "    X, Y = data\n",
    "    X = X.view(-1, 28 * 28) # transformamos los datos para la red        \n",
    "    pred = model(X)\n",
    "    loss = criterion(pred, Y)\n",
    "    running_loss += loss.item()\n",
    "    avg_loss = running_loss/(i + 1)\n",
    "    probs = F.softmax(pred, 1)\n",
    "    label = torch.argmax(probs, dim=1)\n",
    "    correct += torch.sum(label == Y).item()\n",
    "    total += Y.shape[0]\n",
    "    acc = correct/total\n",
    "        \n",
    "print('accuracy: {}, loss: {}'.format(acc, avg_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python37764bit28dc42f4292c4c44b7b26afe40584d62"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
