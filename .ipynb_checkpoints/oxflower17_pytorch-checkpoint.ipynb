{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "# from imgaug import augmenters as iaa\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {0: 'Daffodil', 1: 'Snowdrop', 2: 'Daisy', 3: 'ColtsFoot', 4: 'Dandelion', \\\n",
    "       5: 'Cowslip', 6: 'Buttercup', 7: 'Windflower', 8: 'Pansy', 9:'LilyValley', \\\n",
    "       10: 'Bluebell', 11: 'Crocus', 12: 'Iris', 13: 'Tigerlily', 14:'Tulip', \\\n",
    "       15: 'Fritillary', 16: 'Sunflower'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partición del conjunto de entrenamiento y validación\n",
    "\n",
    "Para definir los conjuntos de entrenamiento y validación, ocupamos la clase ``ImageFolder``, esta nos genera un objeto que le da la estructura necesaria para el entrenamineto (imagen, etiqueta), recibe la ruta de donde obtendremos las imagenes y se le puede añadir una transformación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolder('./17flowers/jpg', transform=transforms.Resize((224,224)))\n",
    "batch_size = 16\n",
    "num_workers = 0\n",
    "valid_size = .2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos la clase ``Subset`` de ``torch.utils`` para separar los datos en el conjunto de entrenamiento y validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain training indices that will be used for validation\n",
    "n_samples = len(dataset)\n",
    "indices = list(range(n_samples))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * n_samples))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_set = torch.utils.data.Subset(dataset, train_idx)\n",
    "val_set = torch.utils.data.Subset(dataset, valid_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente clase únicamente la definimos para transformar las imagenes del conjunto de validación en tensores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_tf(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image, target = self.dataset[index]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation\n",
    "\n",
    "Tener un conjunto de datos extenso puede ser crucial para el desempeño de una red neuronal, pero muchas veces esto no es posible, una alternativa para mejorar el entrenamiento cuando no se poseen muchos datos es usar *data augmentation*. Una técnica muy común es aplicar una serie de transformaciones de forma aleatoria a cada elemento de un batch, de tal forma que en cada época se puedan obtener cierta variabilidad con respecto a los datos originales. \n",
    "\n",
    "En Pytorch, es posible usar esta tecnica. Existen muchas maneras de hacerlo, puede ocuparse el módulo ``transforms`` que vimos anteriormente o adicionalmente incluir alguna biblioteca para transformar imagenes. En este ejemplo usaremos \n",
    "``imgaug``, este es un módulo muy poderoso para *data augmentation*, contiene:\n",
    "\n",
    "* Más de 60 augmenters y técnicas para modificar imágenes.\n",
    "\n",
    "* Máscaras de segmentación, cajas de frontera, destacamiento de puntos y mapas de calor.\n",
    "\n",
    "* Pipelines para augmentation.\n",
    "\n",
    "Puede consultarse la documentación [aquí](https://imgaug.readthedocs.io/en/latest/). También puede consultarse un ejemplo muy simple donde muestra la integración con Pytorch [aquí](https://towardsdatascience.com/data-augmentation-for-deep-learning-4fe21d1a4eb9).\n",
    "\n",
    "Definimos la siguiente transformación que usaremos para nuestro ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imgaug import augmenters as iaa\n",
    "\n",
    "tfs = transforms.Compose([\n",
    "    iaa.Sequential([\n",
    "    iaa.Sometimes(0.5, iaa.GaussianBlur((0, 3.0))), # apply Gaussian blur with a sigma between 0 and 3 to 50% of the images\n",
    "    # apply one of the augmentations: Dropout or CoarseDropout\n",
    "    iaa.OneOf([\n",
    "        iaa.Dropout((0.01, 0.1), per_channel=0.5), # randomly remove up to 10% of the pixels\n",
    "        iaa.CoarseDropout((0.03, 0.15), size_percent=(0.02, 0.05), per_channel=0.2),\n",
    "    ]),\n",
    "    # apply from 0 to 3 of the augmentations from the list\n",
    "    iaa.SomeOf((0, 3),[\n",
    "        iaa.Sharpen(alpha=(0, 1.0), lightness=(0.75, 1.5)), # sharpen images\n",
    "        iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)), # emboss images\n",
    "        iaa.Fliplr(1.0), # horizontally flip\n",
    "        iaa.Sometimes(0.5, iaa.CropAndPad(percent=(-0.25, 0.25))), # crop and pad 50% of the images\n",
    "        iaa.Sometimes(0.5, iaa.Affine(rotate=5)) # rotate 50% of the images\n",
    "    ])\n",
    "],\n",
    "random_order=True # apply the augmentations in random order\n",
    ").augment_image,\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que el pipeline consiste de una serie de transformaciones que se aplican de forma aleatoria.\n",
    "\n",
    "Será necesario definir una nueva clase llamada ``MapDataset`` que nos permita modificar las \n",
    "muestras durante el entrenamiento. Esta clase modifica el tamaño de nuestro conjunto, lo hace \n",
    "``n_copies`` más grande, la intención es que por cada muestra del conjunto original, el nuevo conjunto \n",
    "preserve una copia y haga tres copias transformadas de la original. De esta forma no sólo tendremos un\n",
    "conjunto más grande, si no que en cada época no necesariamente se tendrán las mismas transformaciones, \n",
    "pero si preservaremos la imagenes originales durante todo el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset, transform=None, n_copies=4):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.n_copies = n_copies\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image, target = self.dataset[index // self.n_copies]\n",
    "        img = np.asarray(image)\n",
    "        if (index % self.n_copies != 0) and self.transform:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = transforms.ToTensor()(img)\n",
    "            \n",
    "        return img, target\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_copies * len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_tf = MapDataset(train_set, tfs)\n",
    "n_train = len(train_set_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set_tf = Dataset_tf(val_set, transforms.ToTensor())\n",
    "n_val = len(val_set_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, prepararamos los conjuntos para el loop de entreanamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set_tf, shuffle = True, batch_size=batch_size,\n",
    "                                           num_workers=num_workers)\n",
    "val_loader = torch.utils.data.DataLoader(val_set_tf, shuffle = True, batch_size=batch_size,\n",
    "                                           num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graficación de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de Modelo\n",
    "\n",
    "Para modelos populares, existen dos formas de definir la arquitectura, la primera es definir la red a partir del módulo ``torch.nn`` como vemos a continuación,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=17):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv_base = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2, bias=False),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            \n",
    "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.fc_base = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256*6*6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_base(x)\n",
    "        x = x.view(x.size(0), 256*6*6)\n",
    "        x = self.fc_base(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "oxModel = AlexNet(num_classes)\n",
    "oxModel = oxModel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 96, 55, 55]          34,848\n",
      "       BatchNorm2d-2           [-1, 96, 55, 55]             192\n",
      "              ReLU-3           [-1, 96, 55, 55]               0\n",
      "         MaxPool2d-4           [-1, 96, 27, 27]               0\n",
      "            Conv2d-5          [-1, 256, 27, 27]         614,400\n",
      "       BatchNorm2d-6          [-1, 256, 27, 27]             512\n",
      "              ReLU-7          [-1, 256, 27, 27]               0\n",
      "         MaxPool2d-8          [-1, 256, 13, 13]               0\n",
      "            Conv2d-9          [-1, 384, 13, 13]         885,120\n",
      "             ReLU-10          [-1, 384, 13, 13]               0\n",
      "           Conv2d-11          [-1, 384, 13, 13]       1,327,488\n",
      "             ReLU-12          [-1, 384, 13, 13]               0\n",
      "           Conv2d-13          [-1, 256, 13, 13]         884,992\n",
      "             ReLU-14          [-1, 256, 13, 13]               0\n",
      "        MaxPool2d-15            [-1, 256, 6, 6]               0\n",
      "          Dropout-16                 [-1, 9216]               0\n",
      "           Linear-17                 [-1, 4096]      37,752,832\n",
      "             ReLU-18                 [-1, 4096]               0\n",
      "          Dropout-19                 [-1, 4096]               0\n",
      "           Linear-20                 [-1, 4096]      16,781,312\n",
      "             ReLU-21                 [-1, 4096]               0\n",
      "           Linear-22                   [-1, 17]          69,649\n",
      "================================================================\n",
      "Total params: 58,351,345\n",
      "Trainable params: 58,351,345\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 14.72\n",
      "Params size (MB): 222.59\n",
      "Estimated Total Size (MB): 237.89\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(oxModel, (3,224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma es ocupar la arquitectura ya denida en ``torchvison.models``,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "mynet = models.alexnet()\n",
    "summary(mynet, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pero la primera forma resulta más cómoda, pues es posible personalizar la aquitectura, de tal forma que podamos agreagar *Batch normalization*, *Dropouyt*, entre otros. En nuestro ejemplo ocuparemos la primera opción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Definición de optimizador y función de costos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(oxModel.parameters(), lr=0.001, weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "EPOCHS = 50\n",
    "\n",
    "train_time = 0\n",
    "epoch_loss = []\n",
    "epoch_acc = []\n",
    "val_loss = []\n",
    "val_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(train_loader, model, optimizer, loss_function, pbar, valid=False):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    if valid: \n",
    "        model.eval()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        X, Y = data\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "        n_update = Y.shape[0]\n",
    "        if not valid:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "        pred = model(X)\n",
    "        loss = loss_function(pred, Y)\n",
    "        if not valid:\n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "            \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        avg_loss = running_loss/(i + 1)\n",
    "        \n",
    "        probs = F.softmax(pred, 1)\n",
    "        label = torch.argmax(probs, dim=1)\n",
    "        correct += torch.sum(label == Y).item()\n",
    "        total += Y.shape[0]\n",
    "        acc = correct/total\n",
    "        \n",
    "        pbar.set_postfix(avg_loss='{:.4f}'.format(avg_loss), acc='{:.4f}'.format(acc))\n",
    "        pbar.update(Y.shape[0])\n",
    "        \n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - training:   0%|          | 0/4352 [00:00<?, ?it/s, acc=0.0, avg_loss=0.0]../torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "Epoch 1/50 - training: 100%|██████████| 4352/4352 [10:16<00:00,  7.06it/s, acc=0.0551, avg_loss=2.8918]\n",
      "Epoch 1/50 - validation: 100%|██████████| 272/272 [00:10<00:00, 25.03it/s, acc=0.0515, avg_loss=2.8361]\n",
      "Epoch 2/50 - training: 100%|██████████| 4352/4352 [10:13<00:00,  7.09it/s, acc=0.0565, avg_loss=2.8332]\n",
      "Epoch 2/50 - validation: 100%|██████████| 272/272 [00:10<00:00, 26.04it/s, acc=0.0478, avg_loss=2.8378]\n",
      "Epoch 3/50 - training: 100%|██████████| 4352/4352 [11:02<00:00,  6.57it/s, acc=0.0584, avg_loss=2.8331]\n",
      "Epoch 3/50 - validation: 100%|██████████| 272/272 [00:10<00:00, 25.52it/s, acc=0.0441, avg_loss=2.8396]\n",
      "Epoch 4/50 - training: 100%|██████████| 4352/4352 [11:27<00:00,  6.33it/s, acc=0.0535, avg_loss=2.8328]\n",
      "Epoch 4/50 - validation: 100%|██████████| 272/272 [00:10<00:00, 25.57it/s, acc=0.0441, avg_loss=2.8408]\n",
      "Epoch 5/50 - training: 100%|██████████| 4352/4352 [14:54<00:00,  4.87it/s, acc=0.0600, avg_loss=2.8328]\n",
      "Epoch 5/50 - validation: 100%|██████████| 272/272 [00:10<00:00, 25.42it/s, acc=0.0441, avg_loss=2.8411]\n",
      "Epoch 6/50 - training: 100%|██████████| 4352/4352 [22:48<00:00,  3.18it/s, acc=0.0607, avg_loss=2.8328]\n",
      "Epoch 6/50 - validation: 100%|██████████| 272/272 [00:49<00:00,  5.53it/s, acc=0.0441, avg_loss=2.8413]\n",
      "Epoch 7/50 - training: 100%|██████████| 4352/4352 [37:13<00:00,  1.95it/s, acc=0.0581, avg_loss=2.8325]\n",
      "Epoch 7/50 - validation: 100%|██████████| 272/272 [01:08<00:00,  3.96it/s, acc=0.0441, avg_loss=2.8416]\n",
      "Epoch 8/50 - training:   6%|▌         | 240/4352 [02:20<42:36,  1.61it/s, acc=0.0542, avg_loss=2.8304]"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "    with tqdm(total = n_train, position=0) as pbar_train:\n",
    "        pbar_train.set_description(f'Epoch {epoch + 1}/'+str(EPOCHS)+' - training')\n",
    "        pbar_train.set_postfix(avg_loss='0.0', acc='0.0')\n",
    "        loss_train, acc_train = training_loop(train_loader, oxModel, optimizer, criterion, pbar_train, valid=False)\n",
    "        train_time +=  time.time() - start_time\n",
    "    with tqdm(total = n_val, position=0) as pbar_val:\n",
    "        pbar_val.set_description(f'Epoch {epoch +1}/'+str(EPOCHS)+' - validation')\n",
    "        pbar_val.set_postfix(avg_loss='0.0', acc='0.0')\n",
    "        loss_val, acc_val = training_loop(val_loader, oxModel, None, criterion, pbar_val, valid=True)\n",
    "    \n",
    "    epoch_loss.append(loss_train)\n",
    "    epoch_acc.append(acc_train)\n",
    "    val_loss.append(loss_val)\n",
    "    val_acc.append(acc_val)\n",
    "\n",
    "print(\"--- %s minutes ---\", train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_acc)\n",
    "plt.plot(val_acc)\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epoch_loss)\n",
    "plt.plot(val_loss)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
